{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from crewai import Agent, Task, Crew, Process\n",
    "from utils.j_utils import load_env_vars, print_json\n",
    "from utils.ui_utils import load_json\n",
    "from langchain_openai import ChatOpenAI\n",
    "import langchain_groq\n",
    "from langchain_groq import ChatGroq\n",
    "from crewai_tools import SerperDevTool, ScrapeWebsiteTool, WebsiteSearchTool, FileReadTool, DirectoryReadTool, MDXSearchTool\n",
    "\n",
    "from crewai_tools import BaseTool\n",
    "secrets_path = '.streamlit/secrets.toml'\n",
    "research_data_path = 'data/research_example.json'\n",
    "customer_support_data_path = 'data/customer_support_example.json'\n",
    "sales_data_path = 'data/sales_example.json'\n",
    "event_data_path = 'data/event_example.json'\n",
    "venue_json_path = \"output/venue_details.json\"\n",
    "instructions_path = \"data/instructions\"\n",
    "financial_data_path = 'data/financial_example.json'\n",
    "fake_resume_data_path = 'data/fake_resume.md'\n",
    "resume_data_path = 'data/resume_example.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "research_data = load_json(research_data_path)\n",
    "customer_support_data = load_json(customer_support_data_path)\n",
    "sales_data = load_json(sales_data_path)\n",
    "events_data = load_json(event_data_path)\n",
    "financial_data = load_json(financial_data_path)\n",
    "resume_data = load_json(resume_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPEN AI DATA\n",
    "model = \"gpt-3.5-turbo\"\n",
    "api_key = load_env_vars(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(openai_api_key=api_key, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GROQ DATA\n",
    "model = \"llama3-70b-8192\"\n",
    "api_key = load_env_vars(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(groq_api_key=api_key, model_name=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_tool = ScrapeWebsiteTool(\n",
    "    website_url=\"https://docs.crewai.com/how-to/Creating-a-Crew-and-kick-it-off/\"\n",
    ")\n",
    "serper_api_key = load_env_vars(\"SERPER_API_KEY\")\n",
    "file_read_tool = FileReadTool()\n",
    "search_tool = SerperDevTool()\n",
    "\n",
    "class SentimentAnalysisTool(BaseTool):\n",
    "    name: str =\"Sentiment Analysis Tool\"\n",
    "    description: str = (\"Analyzes the sentiment of text \"\n",
    "         \"to ensure positive and engaging communication.\")\n",
    "    \n",
    "    def _run(self, text: str) -> str:\n",
    "        # Your custom code tool goes here\n",
    "        return \"positive\"\n",
    "    \n",
    "sentiment_analysis_tool = SentimentAnalysisTool()\n",
    "scrape_tool = ScrapeWebsiteTool()\n",
    "instructions_path = \"data/instructions\"\n",
    "directory_read_tool = DirectoryReadTool(directory=instructions_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planner Crew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = Agent(\n",
    "    role=research_data['agents'][0]['role'],\n",
    "    goal=research_data['agents'][0]['goal'],\n",
    "    backstory=research_data['agents'][0]['backstory'],\n",
    "    allow_delegation=False,\n",
    "    verbose=True, \n",
    "    llm=llm\n",
    ")\n",
    "writer = Agent(\n",
    "    role=research_data['agents'][1]['role'],\n",
    "    goal=research_data['agents'][1]['goal'],\n",
    "    backstory=research_data['agents'][1]['backstory'],\n",
    "    allow_delegation=False,\n",
    "    verbose=True, \n",
    "    llm=llm\n",
    ")\n",
    "editor = Agent(\n",
    "    role=research_data['agents'][2]['role'],\n",
    "    goal=research_data['agents'][2]['goal'],\n",
    "    backstory=research_data['agents'][2]['backstory'],\n",
    "    allow_delegation=False,\n",
    "    verbose=True, \n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = Task(\n",
    "    description=research_data['tasks'][0]['description'],\n",
    "    expected_output=research_data['tasks'][0]['expected_output'],\n",
    "    agent=planner\n",
    ")\n",
    "write = Task(\n",
    "    description=research_data['tasks'][1]['description'],\n",
    "    expected_output=research_data['tasks'][1]['expected_output'],\n",
    "    agent=writer\n",
    ")\n",
    "edit= Task(\n",
    "    description=research_data['tasks'][2]['description'],\n",
    "    expected_output=research_data['tasks'][2]['expected_output'],\n",
    "    agent=editor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = Crew(\n",
    "    agents=[planner, writer, editor],\n",
    "    tasks=[plan, write, edit],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = crew.kickoff(\n",
    "    inputs={\"topic\": \"Artificial Intelligents\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model: {model}\")\n",
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Support System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_agent = Agent(\n",
    "    role=customer_support_data['agents'][0]['role'],\n",
    "    goal=customer_support_data['agents'][0]['goal'],\n",
    "    backstory=customer_support_data['agents'][0]['backstory'],\n",
    "    allow_delegation=False,\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")\n",
    "support_quality_agent = Agent(\n",
    "    role=customer_support_data['agents'][1]['role'],\n",
    "    goal=customer_support_data['agents'][1]['goal'],\n",
    "    backstory=customer_support_data['agents'][1]['backstory'],\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inquiry_task = Task(\n",
    "    description=customer_support_data['tasks'][0]['description'],\n",
    "    expected_output=customer_support_data['tasks'][0]['expected_output'],\n",
    "    tools=[scrape_tool],\n",
    "    agent=support_agent\n",
    ")\n",
    "quality_assurance_review = Task(\n",
    "    description=customer_support_data['tasks'][1]['description'],\n",
    "    expected_output=customer_support_data['tasks'][1]['expected_output'],\n",
    "    agent=support_quality_agent\n",
    ")\n",
    "\n",
    "support_crew = Crew(\n",
    "    agents=[support_agent, support_quality_agent],\n",
    "    tasks=[inquiry_task, quality_assurance_review],\n",
    "    verbose=2, \n",
    "    memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    \"customer\": \"DeepLearningAI\",\n",
    "    \"person\": \"Andrew Ng\",\n",
    "    \"inquiry\": \"I need help with setting up a Crew \"\n",
    "               \"and kicking it off, specifically \"\n",
    "               \"how can I add memory to my crew? \"\n",
    "               \"Can you provide guidance?\"\n",
    "}\n",
    "support_result = support_crew.kickoff(inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(support_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIABLES UPDATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o\"\n",
    "llm = ChatOpenAI(openai_api_key=api_key, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sales_rep = Agent(\n",
    "    role=sales_data['agents'][0]['role'],\n",
    "    goal=sales_data['agents'][0]['goal'],\n",
    "    backstory=sales_data['agents'][0]['backstory'],\n",
    "    allow_delegation=False,\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "lead_sales_rep = Agent(\n",
    "    role=sales_data['agents'][1]['role'],\n",
    "    goal=sales_data['agents'][1]['goal'],\n",
    "    backstory=sales_data['agents'][1]['backstory'],\n",
    "    allow_delegation=False,\n",
    "    verbose=True,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiling_task = Task(\n",
    "    description=sales_data['tasks'][0]['description'],\n",
    "    expected_output=sales_data['tasks'][0]['expected_output'],\n",
    "    tools=[directory_read_tool, file_read_tool, search_tool],\n",
    "    agent=sales_rep\n",
    ")\n",
    "\n",
    "outrich_task = Task(\n",
    "    description=sales_data['tasks'][1]['description'],\n",
    "    expected_output=sales_data['tasks'][1]['expected_output'],\n",
    "    tools=[sentiment_analysis_tool, search_tool],\n",
    "    agent=lead_sales_rep\n",
    ")\n",
    "\n",
    "sales_crew = Crew(\n",
    "    agents=[sales_rep, lead_sales_rep],\n",
    "    tasks=[profiling_task, outrich_task],\n",
    "    verbose=2,\n",
    "\tmemory=True\n",
    ")\n",
    "\n",
    "inputs = {\n",
    "    \"lead_name\": \"DeepLearningAI\",\n",
    "    \"industry\": \"Online Learning Platform\",\n",
    "    \"key_decision_maker\": \"Andrew Ng\",\n",
    "    \"position\": \"CEO\",\n",
    "    \"milestone\": \"product launch\"\n",
    "}\n",
    "\n",
    "sales_result = sales_crew.kickoff(inputs=inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(sales_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_coordinator = Agent(\n",
    "    role=events_data['agents'][0]['role'],\n",
    "    goal=events_data['agents'][0]['goal'],\n",
    "    backstory=events_data['agents'][0]['backstory'],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    tools=[search_tool, scrape_tool]\n",
    ")\n",
    "\n",
    "logistics_coordinator = Agent(\n",
    "    role=events_data['agents'][1]['role'],\n",
    "    goal=events_data['agents'][1]['goal'],\n",
    "    backstory=events_data['agents'][1]['backstory'],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    tools=[search_tool, scrape_tool]\n",
    ")\n",
    "\n",
    "marketing_coordinator = Agent(\n",
    "    role=events_data['agents'][2]['role'],\n",
    "    goal=events_data['agents'][2]['goal'],\n",
    "    backstory=events_data['agents'][2]['backstory'],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    tools=[search_tool, scrape_tool]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "# Define a Pydantic model for venue details \n",
    "# (demonstrating Output as Pydantic)\n",
    "class VenueDetails(BaseModel):\n",
    "    name: str\n",
    "    address: str\n",
    "    capacity: int\n",
    "    booking_status: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venue_task = Task(\n",
    "    description=events_data['tasks'][0]['description'],\n",
    "    expected_output=events_data['tasks'][0]['expected_output'],\n",
    "    agent=venue_coordinator,\n",
    "    human_input=True, \n",
    "    output_json=VenueDetails,\n",
    "    output_file=venue_json_path\n",
    ")\n",
    "\n",
    "logistics_task = Task(\n",
    "    description=events_data['tasks'][1]['description'],\n",
    "    expected_output=events_data['tasks'][1]['expected_output'],\n",
    "    agent=logistics_coordinator,\n",
    "    human_input=True,\n",
    "    async_execution=True\n",
    ")\n",
    "marketing_task = Task(\n",
    "    description=events_data['tasks'][2]['description'],\n",
    "    expected_output=events_data['tasks'][2]['expected_output'],\n",
    "    agent=marketing_coordinator,\n",
    "    async_execution=True, \n",
    "    output_file=\"marketing_report.md\"\n",
    ")\n",
    "\n",
    "event_crew = Crew(\n",
    "    agents=[venue_coordinator, logistics_coordinator, marketing_coordinator],\n",
    "    tasks=[venue_task, logistics_task, marketing_task],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "event_details = {\n",
    "    'event_topic': \"Tech Innovation Conference\",\n",
    "    'event_description': \"A gathering of tech innovators \"\n",
    "                         \"and industry leaders \"\n",
    "                         \"to explore future technologies.\",\n",
    "    'event_city': \"San Francisco\",\n",
    "    'tentative_date': \"2024-09-15\",\n",
    "    'expected_participants': 500,\n",
    "    'budget': 20000,\n",
    "    'venue_type': \"Conference Hall\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_results = event_crew.kickoff(inputs=event_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_json(venue_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(\"marketing_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analyst_agent = Agent(\n",
    "    role=financial_data['agents'][0]['role'],\n",
    "    goal=financial_data['agents'][0]['goal'],\n",
    "    backstory=financial_data['agents'][0]['backstory'],\n",
    "    verbose=True,\n",
    "    allow_delegation=True,\n",
    "    tools=[search_tool, scrape_tool],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "trading_strategy_agent = Agent(\n",
    "    role=financial_data['agents'][1]['role'],\n",
    "    goal=financial_data['agents'][1]['goal'],\n",
    "    backstory=financial_data['agents'][1]['backstory'],\n",
    "    verbose=True,\n",
    "    allow_delegation=True,\n",
    "    tools=[search_tool, scrape_tool],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "execution_agent = Agent(\n",
    "    role=financial_data['agents'][2]['role'],\n",
    "    goal=financial_data['agents'][2]['goal'],\n",
    "    backstory=financial_data['agents'][2]['backstory'],\n",
    "    verbose=True,\n",
    "    allow_delegation=True,\n",
    "    tools=[search_tool, scrape_tool],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "risk_management_agent = Agent(\n",
    "    role=financial_data['agents'][3]['role'],\n",
    "    goal=financial_data['agents'][3]['goal'],\n",
    "    backstory=financial_data['agents'][3]['backstory'],\n",
    "    verbose=True,\n",
    "    allow_delegation=True,\n",
    "    tools=[search_tool, scrape_tool],\n",
    "    llm=llm\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_task = Task(\n",
    "    description=financial_data['tasks'][0]['description'],\n",
    "    expected_output=financial_data['tasks'][0]['expected_output'],\n",
    "    agent=data_analyst_agent\n",
    ")\n",
    "\n",
    "strategy_task = Task(\n",
    "    description=financial_data['tasks'][1]['description'],\n",
    "    expected_output=financial_data['tasks'][1]['expected_output'],\n",
    "    agent=trading_strategy_agent\n",
    ")\n",
    "\n",
    "execution_task = Task(\n",
    "    description=financial_data['tasks'][2]['description'],\n",
    "    expected_output=financial_data['tasks'][2]['expected_output'],\n",
    "    agent=execution_agent\n",
    ")\n",
    "\n",
    "risk_task = Task(\n",
    "    description=financial_data['tasks'][3]['description'],\n",
    "    expected_output=financial_data['tasks'][3]['expected_output'],\n",
    "    agent=risk_management_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_crew = Crew(\n",
    "    agents=[data_analyst_agent, trading_strategy_agent, execution_agent, risk_management_agent],\n",
    "    tasks=[data_task, strategy_task, execution_task, risk_task],\n",
    "    process=Process.hierarchical,\n",
    "    manager_llm=ChatOpenAI(openai_api_key=api_key, model=model, temperature=0.7),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "financial_trading_inputs = {\n",
    "    'stock_selection': 'AAPL',\n",
    "    'initial_capital': '100000',\n",
    "    'risk_tolerance': 'Medium',\n",
    "    'trading_strategy_preference': 'Day Trading',\n",
    "    'news_impact_consideration': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financial_result = financial_crew.kickoff(inputs=financial_trading_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(financial_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Builder Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_tool = ScrapeWebsiteTool()\n",
    "read_resume = FileReadTool(file_path=resume_data_path)\n",
    "semantic_search_resume = MDXSearchTool(mdx=resume_data_path)\n",
    "output_path_resume = \"output/tailored_resume.md\"\n",
    "output_path_interview_prep = \"output/interview_prep.md\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "researcher_agent = Agent(\n",
    "    role=resume_data['agents'][0]['role'],\n",
    "    goal=resume_data['agents'][0]['goal'],\n",
    "    backstory=resume_data['agents'][0]['backstory'],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    tools=[scrape_tool, search_tool]\n",
    ")\n",
    "\n",
    "profiler_agent = Agent(\n",
    "    role=resume_data['agents'][1]['role'],\n",
    "    goal=resume_data['agents'][1]['goal'],\n",
    "    backstory=resume_data['agents'][1]['backstory'],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    tools=[scrape_tool, search_tool, read_resume, semantic_search_resume]\n",
    ")\n",
    "\n",
    "resulme_strategist_agent = Agent(\n",
    "    role=resume_data['agents'][2]['role'],\n",
    "    goal=resume_data['agents'][2]['goal'],\n",
    "    backstory=resume_data['agents'][2]['backstory'],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    tools=[scrape_tool, search_tool, read_resume, semantic_search_resume]\n",
    ")\n",
    "\n",
    "interview_preparation_agent = Agent(\n",
    "    role=resume_data['agents'][3]['role'],\n",
    "    goal=resume_data['agents'][3]['goal'],\n",
    "    backstory=resume_data['agents'][3]['backstory'],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    tools=[scrape_tool, search_tool, read_resume, semantic_search_resume]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_task = Task(\n",
    "    description=resume_data['tasks'][0]['description'],\n",
    "    expected_output=resume_data['tasks'][0]['expected_output'],\n",
    "    agent=researcher_agent, \n",
    "    async_execution=True\n",
    ")\n",
    "profile_task = Task(\n",
    "    description=resume_data['tasks'][1]['description'],\n",
    "    expected_output=resume_data['tasks'][1]['expected_output'],\n",
    "    agent=profiler_agent,\n",
    "    async_execution=True\n",
    ")\n",
    "resume_strategy_task = Task(\n",
    "    description=resume_data['tasks'][2]['description'],\n",
    "    expected_output=resume_data['tasks'][2]['expected_output'],\n",
    "    agent=resulme_strategist_agent,\n",
    "    output_file=output_path_resume,\n",
    "    context=[research_task, profile_task],\n",
    ")\n",
    "\n",
    "interview_prep_task = Task(\n",
    "    description=resume_data['tasks'][3]['description'],\n",
    "    expected_output=resume_data['tasks'][3]['expected_output'],\n",
    "    agent=interview_preparation_agent,\n",
    "    output_file=output_path_interview_prep,\n",
    "    context=[research_task, profile_task, resume_strategy_task],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_application_crew = Crew(\n",
    "    agents=[researcher_agent, profiler_agent, resulme_strategist_agent, interview_preparation_agent],\n",
    "    tasks=[research_task, profile_task, resume_strategy_task, interview_prep_task],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_application_inputs = {\n",
    "    'job_posting_url': 'https://jobs.lever.co/AIFund/6c82e23e-d954-4dd8-a734-c0c2c5ee00f1?lever-origin=applied&lever-source%5B%5D=AI+Fund',\n",
    "    'github_url': 'https://github.com/joaomdmoura',\n",
    "    'personal_writeup': \"\"\"Noah is an accomplished Software\n",
    "    Engineering Leader with 18 years of experience, specializing in\n",
    "    managing remote and in-office teams, and expert in multiple\n",
    "    programming languages and frameworks. He holds an MBA and a strong\n",
    "    background in AI and data science. Noah has successfully led\n",
    "    major tech initiatives and startups, proving his ability to drive\n",
    "    innovation and growth in the tech industry. Ideal for leadership\n",
    "    roles that require a strategic and innovative approach.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_application_result = job_application_crew.kickoff(inputs=job_application_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
